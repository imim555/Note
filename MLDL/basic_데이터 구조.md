[출처]
- https://wikidocs.net/52460
- https://chonchony.tistory.com/entry/%ED%96%89%EB%A0%AC-%EA%B3%B1%EC%85%88-vs-%EC%95%84%EB%8B%A4%EB%A7%88%EB%A5%B4-%EA%B3%B1%EC%85%88-Hadamard-product-vs-Matrix-multiplication


# 데이터를 tensor 형태로 이해하기


### 데이터와 행렬

머신 러닝에서는 데이터를 셀 수 있는 단위로 구분할 때, 각각을 샘플이라고 부르며, 종속 변수 y를 예측하기 위한 각각의 독립 변수 $X_i$를 특성(feature)이라 한다


![[Pasted image 20250302225359.png]]


### 파라미터와 행렬
파라미터는 가중치행렬W과 편향행렬B로 구분된다. 입력행렬의 크기와 출력행렬의 크기로부터 W행렬과 B행렬의 크기를 추론할 수 있어야 한다

![[Pasted image 20250302225431.png]]
![[Pasted image 20250302225434.png]]

덧셈 노드인 B행렬은 Y행렬크기에 영향을 주지 않는다. 즉, B행렬크기와 Y행렬크기는 동일하다.







### 3D 텐서의 이해

학습에 투입되는 데이터 X개가 있다고 하자.
X개의 데이터들은 데이터 배열 $\{X_i\}$이다.
이 배열은 (N,T,H) shape 을 가진 3D텐서로 표현할 수 있고, 이때 원소는 NxTxH=X개이다.
N,T,H의 값은 사용자가 임의로 결정할 수 있는 하이퍼파라미터이다.
이 원소를 모두 학습하면 1epoch이다
1epoch = X개 = NxTx1 인경우 -> T는 iteration(학습에 필요한 배치의 개수)

- N  
	- 3D텐서에서 row의 개수
	- batch 사이즈로 동시에 투입되는 학습량
	- batch
- T 
	- 3D텐서에서 col의 개수 
	- time/legth(text), width(vision)
	- 훈련 데이터의 학습횟수로 순차적으로 모델에 투입된다
	- iteration
- H 
	- 3D텐서에서 depth의 개수
	- depth, demension(text), height(vision)
	- feature, 독립변수, data의 vectorization
	- (text) 1단어가 임베딩층을 거치면 H차원으로 확대되는데, 즉 1단어의 feature벡터
	- (vision) 1픽셀이 가지고 있는 채널 수를 의미


![[Pasted image 20250303103421.png]]



![[Pasted image 20250303103621.png]]
![[Pasted image 20250303103637.png]]

![[Pasted image 20250302182839.png]]



텍스트 데이터에 적용하여 생각해보자.
컴퓨터는 배치 단위로 가져가서 연산을 수행한다. 
최종 배열의 shape은 <span style="background:rgba(240, 107, 5, 0.2)">(batch size, 문장 길이, 단어 벡터의 차원)</span>를 의미한다. 

```python 

# corpus : 4개의 문장으로 구성된 전체 훈련 데이터
[[나는 사과를 좋아해], [나는 바나나를 좋아해], [나는 사과를 싫어해], [나는 바나나를 싫어해]]


# word tokenization
[['나는', '사과를', '좋아해'], ['나는', '바나나를', '좋아해'], ['나는', '사과를', '싫어해'], ['나는', '바나나를', '싫어해']]

# word embedding
'나는' = [0.1, 0.2, 0.9] 
'사과를' = [0.3, 0.5, 0.1] 
'바나나를' = [0.3, 0.5, 0.2] 
'좋아해' = [0.7, 0.6, 0.5] 
'싫어해' = [0.5, 0.6, 0.7]

```


``` python

# 훈련데이터 재구성 
# batch=4 => (4,3,3)
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]

# batch=2 => (2,6,3)
# 첫번째 배치 
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]

# 두번째 배치 
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]

```




